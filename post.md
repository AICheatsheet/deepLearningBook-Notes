# Linear Algebra for deep learning with practical code and visualizations

Reading notes on the Deep Learning book (Notes on the Deep Learning book : Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. MIT press.)

![screenshot1](screenshot1.png)

I'd like to introduce a series of blog post and their companion Python Notebooks gathering notes on the Deep Learning book from Goodfellow, I., Bengio, Y., and Courville, A. (2016). The notes actually cover the chapter 2 on Linear Algebra. I like their introduction to linear algebra because it gives a sense of what is most used in the domain of machine learning and deep learning. It is a great syllabus for anyone who want to get the knowledge of linear algebra needed to experiment with deep learning algorithms or to better understand them.

You can find all the notebooks on [here]().

# Getting started with linear algebra

The idea of this series is to provide details aimed at beginners who wants to understand enough linear algebra to be confortable with deep learning. Indeed, I think that the linear algebra ressource proposed in the Deep Learning book is a bit scarce for beginners. I tried to produce code, examples and drawings on each part of the second chapter of the book in order to add steps that may not be obvious for beginners.

# The use of Python/Numpy

In addition, I noticed that creating and reading examples is really helpful to understand the theory. It is why I built Python notebooks instead or just gather mathematical explanations. The goal is two folds:

1. Provide a starting point to use Python/Numpy to apply linear algebra concepts. And since the final goal is to use linear algebra concepts for deep learning it seems natural to continuously go between theory and code. All you will need is a working Python installation with major librairies like Numpy/Scipy/Matplotlib.

2. Give a more concrete vision of the underlying concepts. I found immensely useful to play and experiment with these notebooks to build my understanding of somewhat complicated theoretical concepts or notations. I hope that reading them will be as useful.

# Syllabus

The syllabus follow exactly the [Deep Learning Book so you can find more details if you can't understand one specific point while you are reading it.

1. [Scalars, Vectors, Matrices and Tensors]()
2. [Multiplying Matrices and Vectors]()
3. [Identity and Inverse Matrices]()
4. [Linear Dependence and Span]()
5. [Norms]()
6. [Special Kinds of Matrices and Vectors]()
7. [Eigendecomposition]()
8. [Singular Value Decomposition]()
9. [The Moore-Penrose Pseudoinverse]()
10. [The Trace Operator]()
11. The Determinant
12. Example: Principal Components Analysis

# Setup



# Enjoy

I tried to be as accurate as I could and double checked all statements. However, it is definitely possible that you encounter errors/misunderstandings/typos/english weirdness... Please report it! It is very valuable! and I will try to fix it. You can send me emails or open issues and pull request in the notebooks gihub.
