{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "parentPath = os.path.abspath(\"..\")\n",
    "if parentPath not in sys.path:\n",
    "    sys.path.insert(0, parentPath)\n",
    "from ress.utils import plotVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# Plot parameters\n",
    "sns.set()\n",
    "# Seven hls color palette\n",
    "current_palette_7 = sns.color_palette(\"hls\", 7)\n",
    "sns.set_palette(current_palette_7)\n",
    "\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (4, 4)\n",
    "plt.rcParams['xtick.major.size'] = 0\n",
    "plt.rcParams['ytick.major.size'] = 0\n",
    "# rcParams.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Avoid inaccurate floating values (for inverse matrices in dot product for instance)\n",
    "# See https://stackoverflow.com/questions/24537791/numpy-matrix-inversion-rounding-errors\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert} \n",
    "\\DeclareMathOperator{\\Tr}{Tr}\n",
    "\\newcommand\\bs[1]{\\boldsymbol{#1}}\n",
    "\\newcommand\\argmin[1]{\\underset{\\bs{#1}}{\\arg\\min}}\n",
    "\\newcommand\\argmax[1]{\\underset{\\bs{#1}}{\\arg\\max}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.12 Example - Principal Components Analysis\n",
    "\n",
    "<span class='pquote'>\n",
    "    This content is part of a series following the chapter 2 on linear algebra from the [Deep Learning Book](http://www.deeplearningbook.org/) by Goodfellow, I., Bengio, Y., and Courville, A. (2016). It aims to provide intuitions/drawings/python code on mathematical theories and is constructed as my understanding of these concepts.\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is the last chapter of the Linear Algebra part of the Deep Learning Book. The goal is to use our previous knoledge from chapters 1 to 11 to study a widely used algorithm: the Principal Components Analysis. Be sure to check previous chapters.\n",
    "\n",
    "Dimensions are a crucial topic in data science. The dimensions are all the features of the dataset. For instance, if you are looking at a dataset containing pieces of music, dimensions could be the genre, the length of the piece, the number of instruments, the presence of a singer etc. You can imagine all these dimensions as different columns. When there is only two dimensions, it is very convenient to plot: you can use the $x$- and $y$-axis. Add color and you can represent a third dimension. It is very similar if you have tens or hundereds of dimensions.\n",
    "\n",
    "When you have that many dimensions it happens that some of them are correlated. For instance, we can reasonably think that the genre dimension will correlate with the instruments dimensions in our previous example. It would be nice to have a way to reduce these dimensions while keeping all the information.\n",
    "\n",
    "The aim of principal components analysis (PCA) is generaly to reduce the number of dimensions of a dataset where dimensions are not completly decorelated. PCA provides us with a new set of dimensions, the principal components (PC). They are ordered: the first PC is the dimension having the largest variance. In addition, each PC is orthogonal to the preceding one. Remember that orthogonal vectors means that their dot product is equal to $0$ (see [2.6]()). This means that each PC is decorelated to the preceding one. It is way better than feature selection where you loose a lot of information.\n",
    "\n",
    "### Example 1.\n",
    "\n",
    "Unit vectors is an example of orthogonal vectors:\n",
    "\n",
    "<img src=\"images/orthoVec.png\" alt=\"orthoVec\" width=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, it is possible to keep only the first PCs and reduce dimensionaly without losing much information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Describing the problem\n",
    "\n",
    "The process is precisely described in the book so I will just add some steps, references and explanations.\n",
    "\n",
    "The problem can expressed as finding a function that convert a set of data points from $\\mathbb{R}^n$ to $\\mathbb{R}^l$. This means that we change the number of dimensions of our dataset. We also need a function that can decode back from the transformed dataset to the initial one.\n",
    "\n",
    "<img src=\"images/problem.png\" alt=\"problem\" width=\"80%\">\n",
    "\n",
    "So the first step is to understand the shape of the data. $x^{(i)}$ is one data point containing $n$ dimensions. Let's have our $m$ data points organized as column vectors (one column per point):\n",
    "\n",
    "$$\n",
    "\\bs{x}=\n",
    "\\begin{bmatrix}\n",
    "    x^{(1)} & x^{(2)} & \\cdots & x^{(m)}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "If we deploy the $n$ dimensions of our data points we will have:\n",
    "\n",
    "$$\n",
    "\\bs{x}=\n",
    "\\begin{bmatrix}\n",
    "    x_1^{(1)} & x_1^{(2)} & \\cdots & x_1^{(m)}\\\\\\\\\n",
    "    x_2^{(1)} & x_2^{(2)} & \\cdots & x_2^{(m)}\\\\\\\\\n",
    "    \\cdots & \\cdots & \\cdots & \\cdots\\\\\\\\\n",
    "    x_n^{(1)} & x_n^{(2)} & \\cdots & x_n^{(m)}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We can also write:\n",
    "\n",
    "$$\n",
    "\\bs{x}=\n",
    "\\begin{bmatrix}\n",
    "    x_1\\\\\\\\\n",
    "    x_2\\\\\\\\\n",
    "    \\cdots\\\\\\\\\n",
    "    x_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$c$ will have the exact same dimensions:\n",
    "\n",
    "$$\n",
    "\\bs{c}=\n",
    "\\begin{bmatrix}\n",
    "    c_1\\\\\\\\\n",
    "    c_2\\\\\\\\\n",
    "    \\cdots\\\\\\\\\n",
    "    c_n\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Adding some constraints: the decoding function\n",
    "\n",
    "The encoding function $f()$ transforms $\\bs{x}$ into $\\bs{c}$ and the decoding function transforms back $\\bs{c}$ into an approximation of $\\bs{x}$. To keep things simple, PCA will respect some constraints:\n",
    "\n",
    "### Constraint 1.\n",
    "\n",
    "The decoding function has to be a simple matrix multiplication:\n",
    "\n",
    "$$\n",
    "g(\\bs{c})=\\bs{Dc}\n",
    "$$\n",
    "\n",
    "By applying the matrix $\\bs{D}$ to the dataset from the new coordinates system we should get back to the initial coordinate system.\n",
    "\n",
    "### Constraint 2.\n",
    "\n",
    "The columns of $\\bs{D}$ must be orthogonal.\n",
    "\n",
    "### Constraint 3.\n",
    "\n",
    "The columns of $\\bs{D}$ must have unit norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the encoding function\n",
    "\n",
    "Important: For now we will consider only **one data point**. Thus we will have the following dimensions for these matrices (note that $\\bs{x}$ and $\\bs{c}$ are column vectors):\n",
    "\n",
    "<img src=\"images/decoding.png\" alt=\"decoding\" width=\"250\">\n",
    "\n",
    "We want a decoding function that is a simple matrix multiplication. For that reason, we have $g(\\bs{c})=\\bs{Dc}$. We will find the encoding function from the decoding function. We want to minimize the error between the decoded data point and the actual data point. With our previous notation, this means reducing the distance between $\\bs{x}$ and $g(\\bs{c})$. As an indicator of this distance, we will use the squared $L^2$ norm (see [2.5]()):\n",
    "\n",
    "$$\n",
    "\\norm{\\bs{x} - g(\\bs{c})}_2^2\n",
    "$$\n",
    "\n",
    "This is what we want to minimize. Let's call $\\bs{c}^*$ the optimal $\\bs{c}$. Mathematically it can be written:\n",
    "\n",
    "$$\n",
    "\\bs{c}^* = \\underset{c}{\\arg\\min} \\norm{\\bs{x} - g(\\bs{c})}_2^2\n",
    "$$\n",
    "\n",
    "This means that we want to find the values of the vector $\\bs{c}$ such that $\\norm{\\bs{x} - g(\\bs{c})}_2^2$ is as small as possible.\n",
    "\n",
    "If you have a look back to [2.5]() you can see that the squared $L^2$ norm can be expressed as:\n",
    "\n",
    "$$\n",
    "\\norm{\\bs{y}}_2^2 = \\bs{y}^\\text{T}\\bs{y}\n",
    "$$\n",
    "\n",
    "We named the variable $\\bs{y}$ to avoid confusion with our $\\bs{x}$. Here $\\bs{y}=\\bs{x} - g(\\bs{c})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the equation that we want to minimize becomes:\n",
    "\n",
    "$$\n",
    "(\\bs{x} - g(\\bs{c}))^\\text{T}(\\bs{x} - g(\\bs{c}))\n",
    "$$\n",
    "\n",
    "Since the transpose respects addition we have:\n",
    "\n",
    "$$\n",
    "(\\bs{x}^\\text{T} - g(\\bs{c})^\\text{T})(\\bs{x} - g(\\bs{c}))\n",
    "$$\n",
    "\n",
    "By the distributive property (see [2.2]()) we can develop:\n",
    "\n",
    "$$\n",
    "\\bs{x^\\text{T}x} - \\bs{x}^\\text{T}g(\\bs{c}) -  g(\\bs{c})^\\text{T}\\bs{x} + g(\\bs{c})^\\text{T}g(\\bs{c})\n",
    "$$\n",
    "\n",
    "The commutative property (see [2.2]()) tells us that:\n",
    "\n",
    "$$\n",
    "\\bs{x^\\text{T}y} = \\bs{y^\\text{T}x}\n",
    "$$\n",
    "\n",
    "We can use that in the previous equation:\n",
    "\n",
    "$$\n",
    "\\bs{x}^\\text{T}g(\\bs{c}) = g(\\bs{c})^\\text{T}\\bs{x}\n",
    "$$\n",
    "\n",
    "So it becomes:\n",
    "\n",
    "$$\n",
    "\\bs{x^\\text{T}x} -2\\bs{x}^\\text{T}g(\\bs{c}) + g(\\bs{c})^\\text{T}g(\\bs{c})\n",
    "$$\n",
    "\n",
    "The first term $\\bs{x^\\text{T}x}$ does not depends on $\\bs{c}$ and since we want to minimize the function according to $\\bs{c}$ we can just get off this term. We simplify to:\n",
    "\n",
    "$$\n",
    "\\bs{c}^* = \\underset{c}{\\arg\\min} -2\\bs{x}^\\text{T}g(\\bs{c}) + g(\\bs{c})^\\text{T}g(\\bs{c})\n",
    "$$\n",
    "\n",
    "Since $g(\\bs{c})=\\bs{Dc}$:\n",
    "\n",
    "$$\n",
    "\\bs{c}^* = \\underset{c}{\\arg\\min} -2\\bs{x}^\\text{T}\\bs{Dc} + (\\bs{Dc})^\\text{T}\\bs{Dc}\n",
    "$$\n",
    "\n",
    "With $(\\bs{Dc})^\\text{T}=\\bs{c}^\\text{T}\\bs{D}^\\text{T}$ (see [2.2]()). So we have:\n",
    "\n",
    "$$\n",
    "\\bs{c}^* = \\underset{c}{\\arg\\min} -2\\bs{x}^\\text{T}\\bs{Dc} + \\bs{c}^\\text{T}\\bs{D}^\\text{T}\\bs{Dc}\n",
    "$$\n",
    "\n",
    "As we saw in [2.6](), $\\bs{D}^\\text{T}\\bs{D}=\\bs{I}_l$ because $\\bs{D}$ is orthogonal (actually, it is [semi-orthogonal](https://en.wikipedia.org/wiki/Semi-orthogonal_matrix) if $n \\neq l$) and their columns have unit norm. We can replace in the equation:\n",
    "\n",
    "$$\n",
    "\\bs{c}^* = \\underset{c}{\\arg\\min} -2\\bs{x}^\\text{T}\\bs{Dc} + \\bs{c}^\\text{T}\\bs{I}_l\\bs{c}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\bs{c}^* = \\underset{c}{\\arg\\min} -2\\bs{x}^\\text{T}\\bs{Dc} + \\bs{c}^\\text{T}\\bs{c}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizing the function\n",
    "\n",
    "So far so good! Now the goal is to find the minimum of the function $- 2\\bs{x}^\\text{T}\\bs{Dc} + \\bs{c}^\\text{T}\\bs{c}$. One widely used way of doing that is to use the **gradient descent** algorithm. It is not the focus of this chapter but we will say a word about it (see [4.3]() of the Deep Learning Book for more details). The main idea is that the sign of the derivative of the function at a specific value of $x$ tells you if you need to increase or decrease $x$ to reach the minimum. When the slope is near $0$, the minimum should have been reached.\n",
    "\n",
    "<img src=\"images/gradientDescent.png\" alt=\"gradientDescent\" width=\"400\">\n",
    "\n",
    "However, functions with local minima can trouble the descent:\n",
    "\n",
    "<img src=\"images/localMinima.png\" alt=\"localMinima\" width=\"400\">\n",
    "\n",
    "These examples are in 2 dimensions but the principle stands for higher dimensional functions. The gradient is a vector containing the partial derivatives of all dimensions. Its mathematical notation is $\\nabla_xf(\\bs{x})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the gradient of the function\n",
    "\n",
    "Here we want to minimize through each dimension of $\\bs{c}$. We are looking for a slope of $0$. The equation is:\n",
    "\n",
    "$$\n",
    "\\nabla_c(-2\\bs{x}^\\text{T}\\bs{Dc} + \\bs{c}^\\text{T}\\bs{c})=0\n",
    "$$\n",
    "\n",
    "Let's take these terms separately to calculate the derivative according to $\\bs{c}$. \n",
    "\n",
    "$$\n",
    "\\frac{d(-2\\bs{x}^\\text{T}\\bs{Dc})}{d\\bs{c}} = -2\\bs{x}^\\text{T}\\bs{D}\n",
    "$$\n",
    "\n",
    "The second term is $\\bs{c}^\\text{T}\\bs{c}$. We can develop the vector $\\bs{c}$ and calculate the derivative for each element:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{d(\\bs{c}\\text{T}\\bs{c})}{d\\bs{c}} &=\n",
    "\\left(\\frac{d(\\bs{c}_1^2 + \\bs{c}_2^2 + \\cdots + \\bs{c}_l^2)}{d\\bs{c}_1},\n",
    "\\frac{d(\\bs{c}_1^2 + \\bs{c}_2^2 + \\cdots + \\bs{c}_l^2)}{d\\bs{c}_2},\n",
    "\\cdots,\n",
    "\\frac{d(\\bs{c}_1^2 + \\bs{c}_2^2 + \\cdots + \\bs{c}_l^2)}{d\\bs{c}_l}\\right) \\\\\\\\\n",
    "&=(2\\bs{c}_1, 2\\bs{c}_2, \\cdots, 2\\bs{c}_l)\\\\\\\\\n",
    "&=2(\\bs{c}_1, \\bs{c}_2, \\cdots, \\bs{c}_l)\\\\\\\\\n",
    "&=2\\bs{c}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So we can progress in our derivatives:\n",
    "\n",
    "$$\n",
    "\\nabla_c(-2\\bs{x}^\\text{T}\\bs{Dc} + \\bs{c}^\\text{T}\\bs{c})=0\\\\\\\\\n",
    "-2\\bs{x}^\\text{T}\\bs{D} + 2\\bs{c}=0\\\\\\\\\n",
    "-2\\bs{D}^\\text{T}\\bs{x} + 2\\bs{c}=0\\\\\\\\\n",
    "\\bs{c}=\\bs{D}^\\text{T}\\bs{x}\n",
    "$$\n",
    "\n",
    "Great! We found the encoding function! Here are its dimensions:\n",
    "\n",
    "<img src=\"images/encoding.png\" alt=\"encoding\" width=\"250\">\n",
    "\n",
    "\n",
    "To go back from $\\bs{c}$ to $\\bs{x}$ we use $g(\\bs{c})=\\bs{Dc}$:\n",
    "\n",
    "$$\n",
    "r(\\bs{x}) = g(f(\\bs{x})=\\bs{D}\\bs{D}^\\text{T}\\bs{x}\n",
    "$$\n",
    "\n",
    "<img src=\"images/reconstruction.png\" alt=\"reconstruction\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding $\\bs{D}$\n",
    "\n",
    "The next step is to find the matrix $\\bs{D}$. Recall that the purpose of the PCA is to change the coordinate system in order to maximize the variance along the first dimensions of the projected space. This is equivalent to minimizing the error between data points and their reconstruction ([cf here](https://stats.stackexchange.com/questions/32174/pca-objective-function-what-is-the-connection-between-maximizing-variance-and-m)). See bellow the covariance matrix to have more details.\n",
    "\n",
    "<blockquote>\n",
    "Maximizing the variance corresponds to minimizing the error of the reconstruction.\n",
    "</blockquote>\n",
    "\n",
    "### The Frobenius norm\n",
    "\n",
    "Since we have to take all points into account (the same matrix $\\bs{D}$ will be used for all points) we will use the Frobenius norm of the errors (see [2.5]()) which is the equivalent of the $L^2$ norm for matrices. Here the formula of the Frobenius norm:\n",
    "\n",
    "$$\n",
    "\\norm{\\bs{A}}_F=\\sqrt{\\sum_{i,j}A^2_{i,j}}\n",
    "$$\n",
    "\n",
    "It is like if you unroll the matrix to end up with a one dimensional vector and that you take the $L^2$ norm of this vector.\n",
    "\n",
    "We will call $\\bs{D}^*$ the optimal $\\bs{D}$ (in the sense that the error is as small as possible). We have:\n",
    "\n",
    "$$\n",
    "\\bs{D}^* = \\underset{\\bs{D}}{\\arg\\min} \\sqrt{\\sum_{i,j}(x_j^{(i)}-r(\\bs{x}^{(i)})_j})^2\n",
    "$$\n",
    "\n",
    "With the constraint that $\\bs{D}^\\text{T}\\bs{D}=\\bs{I}_l$ because we choose the constraint of having the columns of $\\bs{D}$ orthogonal.\n",
    "\n",
    "### The first principal component\n",
    "\n",
    "We will start to find only the first principal component (PC). For that reason, we will have $l=1$. So the matrix $\\bs{D}$ will have the shape $(n \\times 1)$: it is a simple column vector. Since it is a vector we will call it $\\bs{d}$:\n",
    "\n",
    "<img src=\"images/l1.png\" alt=\"l1\" width=\"100\">\n",
    "\n",
    "We can therefore remove the sum over $j$ and the square root since we will take the squared $L^2$ norm:\n",
    "\n",
    "$$\n",
    "\\bs{d}^* = \\underset{\\bs{d}}{\\arg\\min} \\sum_{i}\\norm{(\\bs{x}^{(i)}-r(\\bs{x}^{(i)}))}_2^2\n",
    "$$\n",
    "\n",
    "\n",
    "We have also seen that:\n",
    "\n",
    "$$\n",
    "r(\\bs{x})=\\bs{D}\\bs{D}^\\text{T}\\bs{x}\n",
    "$$\n",
    "\n",
    "Since we are looking only for the first PC:\n",
    "\n",
    "$$\n",
    "r(\\bs{x})=\\bs{d}\\bs{d}^\\text{T}\\bs{x}\n",
    "$$\n",
    "\n",
    "We can plug $r(\\bs{x})$ into the equation:\n",
    "\n",
    "$$\n",
    "\\bs{d}^* = \\underset{\\bs{d}}{\\arg\\min} \\sum_{i}\\norm{\\bs{x}^{(i)}-\\bs{dd}^\\text{T}\\bs{x}^{(i)}}_2^2\n",
    "$$\n",
    "\n",
    "Because of the constraint 3. (the columns of $\\bs{D}$ have unit norms) we have $\\norm{\\bs{d}}_2 = 1$. $\\bs{d}$ is one of the columns of $\\bs{D}$ and thus has a unit norm.\n",
    "\n",
    "$$\n",
    "\\bs{d}^* = \\underset{\\bs{d}}{\\arg\\min} \\sum_{i}\\norm{\\bs{x}^{(i)}-\\bs{dd}^\\text{T}\\bs{x}^{(i)}}_2^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\bs{d}^* = \\underset{\\bs{d}}{\\arg\\min} \\sum_{i}\\norm{\\bs{x}^{(i)}-\\bs{d}^\\text{T}\\bs{x}^{(i)}\\bs{d}}_2^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\bs{d}^* = \\underset{\\bs{d}}{\\arg\\min} \\sum_{i}\\norm{\\bs{x}^{(i)}-\\bs{x}^{(i)\\text{T}}\\bs{dd}}_2^2\n",
    "$$\n",
    "\n",
    "Instead of using the sum along the $m$ data points $\\bs{x}$ we can have the matrix $\\bs{X}$ which gather all the observations:\n",
    "\n",
    "$$\n",
    "\\bs{X} = \\begin{bmatrix}\n",
    "    \\bs{x}^{(1)\\text{T}}\\\\\\\\\n",
    "    \\bs{x}^{(2)\\text{T}}\\\\\\\\\n",
    "    \\cdots\\\\\\\\\n",
    "    \\bs{x}^{(m)\\text{T}}\n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    "    \\bs{x}_1^{(1)} & \\bs{x}_2^{(1)} & \\cdots & \\bs{x}_n^{(1)}\\\\\\\\\n",
    "    \\bs{x}_1^{(2)} & \\bs{x}_2^{(2)} & \\cdots & \\bs{x}_n^{(2)}\\\\\\\\\n",
    "    \\cdots & \\cdots & \\cdots & \\cdots\\\\\\\\\n",
    "    \\bs{x}_0^{(m)} & \\bs{x}_1^{(m)} & \\cdots & \\bs{x}_n^{(m)}\\\\\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\bs{d}^* = \\underset{\\bs{d}}{\\arg\\min} \\norm{\\bs{X}-\\bs{X}\\bs{dd}^\\text{T}}_\\text{F}^2\n",
    "$$\n",
    "\n",
    "with the constraint that $\\bs{dd}^\\text{T}=1$.\n",
    "\n",
    "### Using the Trace operator\n",
    "\n",
    "We will now use the Trace operator (see [2.10]()) to simplify the equation to minimize. Recall that:\n",
    "\n",
    "$$\n",
    "\\norm{\\bs{A}}_F=\\sqrt{\\Tr({\\bs{AA}^T})}\n",
    "$$\n",
    "\n",
    "So here $\\bs{A}=\\bs{X}-\\bs{X}\\bs{dd}^\\text{T}$. So we have:\n",
    "\n",
    "$$\n",
    "\\bs{d}^* = \\underset{\\bs{d}}{\\arg\\min} \\Tr{((\\bs{X}-\\bs{Xdd}^\\text{T})}(\\bs{X}-\\bs{Xdd}^\\text{T})^\\text{T})\n",
    "$$\n",
    "\n",
    "Since we can cycle the order of the matrices in a Trace (see [2.10]()) we can write:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\bs{d}^* &= \\argmin{d} \\Tr{((\\bs{X}-\\bs{Xdd}^\\text{T})^\\text{T}}(\\bs{X}-\\bs{Xdd}^\\text{T}))\\\\\\\\\n",
    "&=\\argmin{d} \\Tr{((\\bs{X}^\\text{T}-(\\bs{Xdd}^\\text{T})^\\text{T})}(\\bs{X}-\\bs{Xdd}^\\text{T}))\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "And $(\\bs{Xdd}^\\text{T})^\\text{T}=(\\bs{d}^\\text{T})^\\text{T}\\bs{d}^\\text{T}\\bs{X}^\\text{T}=\\bs{d}\\bs{d}^\\text{T}\\bs{X}^\\text{T}$. Let's plug that into our equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\bs{d}^* &= \\argmin{d} \\Tr{(\\bs{X}^\\text{T}-\\bs{d}\\bs{d}^\\text{T}\\bs{X}^\\text{T})}(\\bs{X}-\\bs{Xdd}^\\text{T}))\\\\\\\\\n",
    "&= \\argmin{d} \\Tr{(\\bs{X}^\\text{T}\\bs{X}-\\bs{X}^\\text{T}\\bs{Xdd}^\\text{T} -\\bs{d}\\bs{d}^\\text{T}\\bs{X}^\\text{T}\\bs{X} +\\bs{d}\\bs{d}^\\text{T}\\bs{X}^\\text{T}\\bs{Xdd}^\\text{T}})\\\\\\\\\n",
    "&= \\argmin{d} \\Tr{(\\bs{X}^\\text{T}\\bs{X})} - \\Tr{(\\bs{X}^\\text{T}\\bs{Xdd}^\\text{T})}\n",
    "- \\Tr{(\\bs{d}\\bs{d}^\\text{T}\\bs{X}^\\text{T}\\bs{X})} + \\Tr{(\\bs{d}\\bs{d}^\\text{T}\\bs{X}^\\text{T}\\bs{Xdd}^\\text{T})}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We can remove the first term that not depends on $d$:\n",
    "\n",
    "$$\n",
    "\\bs{d}^* = \\argmin{d} - \\Tr{(\\bs{X}^\\text{T}\\bs{Xdd}^\\text{T})}\n",
    "- \\Tr{(\\bs{d}\\bs{d}^\\text{T}\\bs{X}^\\text{T}\\bs{X})} + \\Tr{(\\bs{d}\\bs{d}^\\text{T}\\bs{X}^\\text{T}\\bs{Xdd}^\\text{T})}\n",
    "$$\n",
    "\n",
    "Still because of the cycling property of a trace, we have\n",
    "\n",
    "$$\n",
    "\\Tr{(\\bs{X}^\\text{T}\\bs{Xdd}^\\text{T})} = \\Tr{(\\bs{d}\\bs{d}^\\text{T}\\bs{X}^\\text{T}\\bs{X})}\n",
    "$$\n",
    "\n",
    "We can simplify to:\n",
    "\n",
    "$$\n",
    "\\bs{d}^* = \\argmin{d} -2\\Tr{(\\bs{X}^\\text{T}\\bs{Xdd}^\\text{T})}\n",
    " + \\Tr{(\\bs{d}\\bs{d}^\\text{T}\\bs{X}^\\text{T}\\bs{Xdd}^\\text{T})}\n",
    "$$\n",
    "\n",
    "and then\n",
    "\n",
    "$$\n",
    "\\bs{d}^* = \\argmin{d} -2\\Tr{(\\bs{X}^\\text{T}\\bs{Xdd}^\\text{T})}\n",
    " + \\Tr{(\\bs{X}^\\text{T}\\bs{Xdd}^\\text{T}\\bs{d}\\bs{d}^\\text{T})}\n",
    "$$\n",
    "\n",
    "Because of the constraint $\\bs{dd}^\\text{T}=1$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\bs{d}^* &= \\argmin{d} -2\\Tr{(\\bs{X}^\\text{T}\\bs{Xdd}^\\text{T})}\n",
    " + \\Tr{(\\bs{X}^\\text{T}\\bs{Xd}\\bs{d}^\\text{T})}\\textrm{ subject to }\\bs{dd}^\\text{T}=1\\\\\\\\\n",
    "&= \\argmin{d} -\\Tr{(\\bs{X}^\\text{T}\\bs{Xdd}^\\text{T})}\\textrm{ subject to }\\bs{dd}^\\text{T}=1\\\\\\\\\n",
    "&=\\argmax{d} \\Tr{(\\bs{X}^\\text{T}\\bs{Xdd}^\\text{T})}\\textrm{ subject to }\\bs{dd}^\\text{T}=1\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and with the cycling property:\n",
    "\n",
    "$$\n",
    "\\bs{d}^* = \\argmax{d} \\Tr{(\\bs{d}^\\text{T}\\bs{X}^\\text{T}\\bs{Xd})} \\textrm{ subject to }\\bs{dd}^\\text{T}=1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigendecomposition\n",
    "\n",
    "We will see that we can find the maximum of the function by calculating the eigenvectors of $\\bs{X^\\text{T}X}$.\n",
    "\n",
    "\n",
    "### Covariance matrix\n",
    "\n",
    "As we wrote above, the optimization problem of maximizing the variance of the components and minimizing the error between the reconstructed and the actual data are equivalent. Actually, if you look at the formula of $\\bs{d}$ you can see that there is the term $\\bs{X\\text{T}X}$ in the middle. It is more clear now that we have the formula of $\\bs{d}^*$.\n",
    "\n",
    "Actually if we have centered our data around 0 (see bellow for more details about centering), $\\bs{X^\\text{T}X}$ is the covariance matrix (see [this Quora question]()).\n",
    "\n",
    "The covariance matrix is a $n$ by $n$ matrix ($n$ being the number of dimensions). Its diagonal is the variance of the corresponding dimensions and the other cells are the covariance between the two corresponding dimensions (the amount of redundancy).\n",
    "\n",
    "This means that the largest covariance we have between two dimensions the more redundancy exists between these dimensions. This also means that the best-fit line is associated with small errors if the covariance is hight. To maximize the variance and minimize the covariance (in order to decorrelate the dimensions) means that the ideal covariance matrix is a diagonal matrix (non-zero values in the diagonal only). Therefore the diagonalization of the covariance matrix will give us the optimal solution.\n",
    "\n",
    "\n",
    "Since we centered the data as a preprocessing the covariance matrix is\n",
    "\n",
    "$$\\bs{C_X}=\\bs{X^\\text{T}X}$$\n",
    "\n",
    "\n",
    "\n",
    "This means that maximizing $\\Tr{(\\bs{d}^\\text{T}\\bs{X}^\\text{T}\\bs{Xd})}$\n",
    "\n",
    "https://arxiv.org/pdf/1404.1100.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Example 2.\n",
    "\n",
    "As an example we will create again a 2D data set (like in [2.9]()). To see the effect of the PCA we will introduce some correlations between the two dimensions. Let's create 100 data points with 2 dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "x = 5*np.random.rand(100)\n",
    "y = 2*x + 1 + np.random.randn(100)\n",
    "\n",
    "x = x.reshape(100, 1)\n",
    "y = y.reshape(100, 1)\n",
    "\n",
    "X = np.hstack([x, y])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10cee2090>]"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQIAAAECCAYAAAAVT9lQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niAjw/4bK6kd1wJR/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c5b7d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X[:,0], X[:,1], '*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highly correlated data means that the dimensions are redundant. It is possible to predict one from the other without losing much information.\n",
    "\n",
    "The first processing we will do is to center the data around 0. PCA is a regression model without intercept and the first component is thus necessarly crossing the origin.\n",
    "\n",
    "Here is a simple function that substract the mean of each column to each data point of this column. It has the effect of centering the data point around 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def centerData(X):\n",
    "    X = X.copy()\n",
    "    X -= np.mean(X, axis = 0)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's center our data $\\bs{X}$ around 0 for both dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10d82a490>]"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQIAAAECCAYAAAAVT9lQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/D+3h3Wf14f0kQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10cde7990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_centered = centerData(X)\n",
    "plt.plot(X_centered[:,0], X_centered[:,1], '*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now look for PCs. We saw that they correspond to values taken by $\\bs{d}$ that maximize the following function:\n",
    "\n",
    "$$\n",
    "\\bs{d}^* = \\argmax{d} \\Tr{(\\bs{d}^\\text{T}\\bs{X}^\\text{T}\\bs{Xd})} \\textrm{ subject to }\\bs{dd}^\\text{T}=1\n",
    "$$\n",
    "\n",
    "To find $\\bs{d}$ we can calculate the eigenvectors of $\\bs{X^\\text{T}X}$ (see [2.7]() for more details about eigendecomposition). So let's do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.91116273, -0.41204669],\n",
       "       [ 0.41204669, -0.91116273]])"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigVals, eigVecs = np.linalg.eig(X_centered.T.dot(X_centered))\n",
    "eigVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the vectors maximizing our function. Each column vector is associated with an eigenvalue. The vector associated with the larger eigenvalue tells us the direction associated with the larger variance in our data. To check that, we will plot these vectors along with the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3, 3)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQIAAAECCAYAAAAVT9lQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEoGIoEQgIigRiAjw/8bZW0CGzb/UAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c575ad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotVectors(eigVecs.T, ['#FF9A13', '#1190FF'])\n",
    "plt.plot(X1[:,0], X1[:,1], '*')\n",
    "plt.xlim(-3, 3)\n",
    "plt.ylim(-3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the blue vector direction corresponds to oblique shape of our data. The idea is that if you project the data points on the line corresponding to the blue vector direction you will have the more spreading of the data. Look at the following figure:\n",
    "\n",
    "<img src=\"images/variance.png\" alt=\"variance\" width=\"400\">\n",
    "\n",
    "When you project data points on the pink line they are really spread. This line has the direction that maximize the variance of the data points. It is the same for the figure above: our blue vector has the direction where data point projection has the higher variance. Then the second eigenvector is orthogonal to the first. It is the second eigenvector so let's check that well the one associated with the bigger eigenvalue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  18.04730409,  798.35242844])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigVals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So yes, the second vector corresponds to bigger eigenvalue.\n",
    "\n",
    "Now that we have found the matrix $\\bs{d}$ we will use the encoding function to rotate the data. The goal of the rotation is to end up with a new coordinate system where data is uncorrelated and then where the basis axes gather all the variance. It is then possible to keep only few axes: this is the purpose of dimensionality reduction.\n",
    "\n",
    "Recall that the encoding function is:\n",
    "\n",
    "$$\n",
    "\\bs{c}=\\bs{D}^\\text{T}\\bs{x}\n",
    "$$\n",
    "\n",
    "$\\bs{D}$ is the eigenvectors we calculated before. In addition, this formula corresponds to only one data point where dimensions are the rows of $\\bs{x}$. In our case, we will apply it for all data points and since $\\bs{X}$ has dimensions on the columns we need to transpose it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5, 5)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAD9CAYAAABTCakVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEmEKACIRpgAgEmEKACIR9v+OoYe9c1LMsgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c0d7ad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_new = eigVecs.T.dot(X_centered.T)\n",
    "\n",
    "plt.plot(eigVecs.T.dot(X_centered.T)[0, :], eigVecs.T.dot(X_centered.T)[1, :], '*')\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked! The rotation transformed our dataset that have now the more variance on one of the basis axis. You could keep only this dimension and have a fairly good representation of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the unit norm constraint\n",
    "\n",
    "We saw that the maximization is subject to $\\bs{dd}^\\text{T}=1$. This means that the solution vector has to be a unit vector. Without this constraint, you could scale $\\bs{d}$ up to the infinity to increase the function to maximize (see [here](https://stats.stackexchange.com/questions/117695/why-is-the-eigenvector-in-pca-taken-to-be-unit-norm)). For instance, let's see some vectors $\\bs{x}$ that could maximize the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4165298.04389264]])"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = np.array([[12], [26]])\n",
    "d.T.dot(X.T).dot(X).dot(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However this $\\bs{d}$ has not a unit norm (since $\\bs{d}$ is a column vector we use the transpose of $\\bs{dd}^\\text{T}$ (see [2.2]()):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[820]])"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.T.dot(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvectors have unit norm and thus respect the constraint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigVecs[:,0].dot(eigVecs[:,0].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigVecs[:,1].dot(eigVecs[:,1].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "https://math.stackexchange.com/questions/1377764/derivative-of-vector-and-vector-transpose-product\n",
    "\n",
    "http://www4.ncsu.edu/~slrace/LinearAlgebra2017/Slides/PCAPrint.pdf\n",
    "\n",
    "https://stats.stackexchange.com/questions/318625/why-do-the-leading-eigenvectors-of-a-maximize-texttrdtad\n",
    "\n",
    "https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c\n",
    "\n",
    "https://www.cs.bgu.ac.il/~inabd171/wiki.files/lecture14_handouts.pdf\n",
    "\n",
    "https://brilliant.org/wiki/principal-component-analysis/#from-approximate-equality-to-minimizing-function\n",
    "\n",
    "\n",
    "\n",
    "https://arxiv.org/pdf/1404.1100.pdf\n",
    "\n",
    "https://www.cs.bgu.ac.il/~inabd171/wiki.files/lecture14_handouts.pdf\n",
    "\n",
    "\n",
    "https://en.wikipedia.org/wiki/Semi-orthogonal_matrix\n",
    "\n",
    "### Intuition about PCA\n",
    "\n",
    "- https://georgemdallas.wordpress.com/2013/10/30/principal-component-analysis-4-dummies-eigenvectors-eigenvalues-and-dimension-reduction/\n",
    "\n",
    "### Link between variance maximized and error minimized:\n",
    "\n",
    "- https://stats.stackexchange.com/questions/130721/what-norm-of-the-reconstruction-error-is-minimized-by-the-low-rank-approximation\n",
    "\n",
    "- https://stats.stackexchange.com/questions/32174/pca-objective-function-what-is-the-connection-between-maximizing-variance-and-m\n",
    "\n",
    "### Centering data\n",
    "\n",
    "- https://www.quora.com/Why-do-we-need-to-center-the-data-for-Principle-Components-Analysis\n",
    "- https://stats.stackexchange.com/questions/22329/how-does-centering-the-data-get-rid-of-the-intercept-in-regression-and-pca\n",
    "\n",
    "### Unit norm constraint\n",
    "\n",
    "- https://stats.stackexchange.com/questions/117695/why-is-the-eigenvector-in-pca-taken-to-be-unit-norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00000001]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2603.84104568]])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = np.array([[-0.9219176], [-0.38738604]])\n",
    "print d.T.dot(d)\n",
    "d.T.dot(X.T).dot(X).dot(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  779.10674359,  1809.61302066],\n",
       "       [ 1809.61302066,  4325.30791113]])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X.T).dot(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   18.71468582,  5085.69996889]), array([[-0.9219176 , -0.38738604],\n",
       "        [ 0.38738604, -0.9219176 ]]))"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.eig((X.T).dot(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
