{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# Plot parameters\n",
    "sns.set()\n",
    "# Seven hls color palette\n",
    "current_palette_7 = sns.color_palette(\"hls\", 7)\n",
    "sns.set_palette(current_palette_7)\n",
    "\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (4, 4)\n",
    "plt.rcParams['xtick.major.size'] = 0\n",
    "plt.rcParams['ytick.major.size'] = 0\n",
    "# rcParams.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Avoid inaccurate floating values (for inverse matrices in dot product for instance)\n",
    "# See https://stackoverflow.com/questions/24537791/numpy-matrix-inversion-rounding-errors\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.12 Example - Principal Components Analysis\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As the last chapter of the linear algebra part of the Deep Learning Book, an example is proposed. Thanks to all the things we saw from chapters 1 to 11, we can now understand the principal components analysis which is a widely used algorithm.\n",
    "\n",
    "The aim of principal components analysis (PCA) is generaly to reduce the number of dimensions of a dataset. PCA provides us with a new set of dimensions that are a linear combination of the initial dimensions. We call these new dimensions the principal components (PC) because they are ordered: the first PC is the dimension having the largest variance. Each PC is also orthogonal to the preceding one. Remember that orthogonal vectors means that their dot product is equal to $0$ (see [2.6]()). This means that each PC is decorelated to the preceding one.\n",
    "\n",
    "### Example 1.\n",
    "\n",
    "Unit vectors is an example of orthogonal vectors:\n",
    "\n",
    "<img src=\"images/orthoVec.png\" alt=\"orthoVec\" width=\"300\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, it is possible to keep only the first PCs and thus reduce dimensionaly without losing much information.\n",
    "\n",
    "The initial dimensions can be partly correlated\n",
    "\n",
    "\n",
    "[semi orthogonal matrix](https://en.wikipedia.org/wiki/Semi-orthogonal_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2 45  1]\n",
      "[[3 1]\n",
      " [5 2]\n",
      " [7 6]]\n",
      "\n",
      "[238  98]\n",
      "\n",
      "[238  98]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([2, 45, 1])\n",
    "print x\n",
    "D = np.array([[3, 1], [5, 2], [7, 6]])\n",
    "print D\n",
    "print '\\n', x.T.dot(D)\n",
    "print '\\n', D.T.dot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1, 0, 0]).dot(np.array([0, 0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Describing the problem\n",
    "\n",
    "The steps are well described in the book but I will add some references and explanations. The problem can expressed as finding a function that convert a set of data points from $\\mathbb{R}^n$ to $\\mathbb{R}^l$. This means that we change the number of dimensions of our dataset. We also need a function that can decode back from the transformed dataset to the initial one.\n",
    "\n",
    "<img src=\"images/encoding.png\" alt=\"encoding\" width=\"80%\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here the encoding function $f()$ transforms $\\boldsymbol{x}$ into $\\boldsymbol{c}$ and the decoding function transforms back $\\boldsymbol{c}$ into an approximation of $\\boldsymbol{x}$.\n",
    "\n",
    "To keep things simple, PCA will respect some constraints:\n",
    "\n",
    "- The decoding function has to be a simple matrix multiplication:\n",
    "\n",
    "$$\n",
    "g(\\boldsymbol{c})=\\boldsymbol{Dc}\n",
    "$$\n",
    "\n",
    "So by applying the matrix $\\boldsymbol{D}$ to the new dataset, we should get back the initial dataset.\n",
    "\n",
    "- The columns of $\\boldsymbol{D}$ have to be orthogonal with a unit norm.\n",
    "\n",
    "One way to evaluate the goodness of the encoding/decoding is to compare $\\boldsymbol{x}$ with the decoded $g(\\boldsymbol{c})$. By comparison, we mean distance. We will use the squared $L^2$ norm (see [2.5]()) to define this distance:\n",
    "\n",
    "$$\n",
    "\\norm{\\boldsymbol{x} - g(\\boldsymbol{c})}_2^2\n",
    "$$\n",
    "\n",
    "This is what we want to minimize. Let's call $\\boldsymbol{c}^*$ the optimal $\\boldsymbol{c}$. Mathematically it can be written:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{c}^* = \\underset{c}{\\arg\\min} \\norm{\\boldsymbol{x} - g(\\boldsymbol{c})}_2^2\n",
    "$$\n",
    "\n",
    "This means that we want to find the values of the vector $\\boldsymbol{c}$ such that $\\norm{\\boldsymbol{x} - g(\\boldsymbol{c})}_2^2$ is as small as possible.\n",
    "\n",
    "If you have a look back to [2.5]() you can see that the squared $L^2$ norm can be expressed as:\n",
    "\n",
    "$$\n",
    "\\norm{\\boldsymbol{x}}_2^2 = \\boldsymbol{x}^\\text{T}\\boldsymbol{x}\n",
    "$$\n",
    "\n",
    "Thus we have\n",
    "\n",
    "$$\n",
    "\\boldsymbol{c}^* = \\underset{c}{\\arg\\min} \\norm{\\boldsymbol{x} - g(\\boldsymbol{c})}_2^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
